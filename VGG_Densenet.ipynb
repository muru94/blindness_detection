{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG_Densenet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDGAwBV9mBcu"
      },
      "source": [
        "#Importing packages\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense,Input,Conv2D,MaxPool2D,Activation,Dropout,Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "import random as rn\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.applications import VGG16\n",
        "import datetime\n",
        "import glob\n",
        "import warnings\n",
        "from tensorflow.keras import models, layers\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import math\n",
        "from keras.regularizers import l1 ,l2\n",
        "import keras\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import argparse\n",
        "from google.colab import files\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "from keras.callbacks import Callback\n",
        "from keras import backend as K\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwTpYO0roHrJ",
        "outputId": "e866ff76-7a80-456e-8dc8-285495f5a007"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y24VNR8oi5s"
      },
      "source": [
        "# Custom method -  to fetch the actual path & name of all the images\n",
        "def abs_path(imgs):\n",
        "    img_path = list()\n",
        "    img_fullname = list()\n",
        "    img_name = list()\n",
        "    for i in imgs:\n",
        "        img_path.append(i)\n",
        "        temp = i.split(\"/\")[-1]\n",
        "        img_fullname.append(temp)\n",
        "        temp = temp.split(\".\")[0]\n",
        "        img_name.append(str(temp))\n",
        "    return img_path, img_fullname, img_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFfWZh1IpAow"
      },
      "source": [
        "images = glob.glob('/content/drive/MyDrive/Colab_Notebooks/Case_study2/preprocessed/*')\n",
        "img_path, img_fullname, img_name = abs_path(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNWFqmTasRt0"
      },
      "source": [
        "pre_df = pd.DataFrame()\n",
        "pre_df['path'] = img_path\n",
        "pre_df['fullname'] = img_fullname\n",
        "pre_df['id_code'] = img_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-MIgb30s4d_"
      },
      "source": [
        "# Reading DR grades files of 2019, Messidor & IDRiD\n",
        "train2019 = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/Case_study2/train.csv')\n",
        "train2019 = train2019[['id_code', 'diagnosis']]\n",
        "messidor = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/Case_study2/messidor_data.csv')\n",
        "messidor = messidor[['id_code', 'diagnosis']]\n",
        "idridtrain = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/Case_study2/IDRID_Training_Labels.csv')\n",
        "idridtrain = idridtrain[['id_code', 'diagnosis']]\n",
        "idridtest = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/Case_study2/IDRID_Testing_Labels.csv')\n",
        "idridtest = idridtest[['id_code', 'diagnosis']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvN6c0aMtYG3"
      },
      "source": [
        "# Fetching DR grades of the respective images of 2019, IDRiD & Messidor data\n",
        "labels = list()\n",
        "id_code = list()\n",
        "for i, j in train2019.iterrows():\n",
        "    id_code.append(j['id_code'])\n",
        "    labels.append(j['diagnosis'])\n",
        "\n",
        "for i, j in messidor.iterrows():\n",
        "    id_code.append(j['id_code'].split(\".\")[0])\n",
        "    labels.append(j['diagnosis'])\n",
        "\n",
        "for i, j in idridtrain.iterrows():\n",
        "    id_code.append(j['id_code'])\n",
        "    labels.append(j['diagnosis'])\n",
        "\n",
        "for i, j in idridtest.iterrows():\n",
        "    id_code.append(j['id_code'])\n",
        "    labels.append(j['diagnosis'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhVz6xM7tbcn"
      },
      "source": [
        "train_labels = pd.DataFrame() # Storing the image file name & DR grades into the dataframe\n",
        "train_labels['id_code'] = id_code\n",
        "train_labels['diagnosis'] = labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_y-86b9tfSj"
      },
      "source": [
        "# Merging the image path & DR grades \n",
        "pre_df = pd.merge(pre_df, train_labels, on='id_code', how='left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqW_rs_yjuYj"
      },
      "source": [
        "pre_df = pre_df[pre_df['diagnosis'].notna()] # removing row if the DR grade is NaN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvr7QlHhjxEr",
        "outputId": "05136178-4dc2-4f2a-a8f1-d00b751de101"
      },
      "source": [
        "print(pre_df[pre_df['diagnosis'].isnull()]) # Verify if there is any NaN present in the DR grades field"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Empty DataFrame\n",
            "Columns: [path, fullname, id_code, diagnosis]\n",
            "Index: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05NdyJGFtxdA"
      },
      "source": [
        "dia_code = list()   # Converting the data type of DR grades field from Float to Int\n",
        "for i, j in pre_df.iterrows():\n",
        "    dia_code.append(int(j['diagnosis']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZWl0TaEuM-I"
      },
      "source": [
        "del pre_df['diagnosis'] # removing the existing DR grades field with data type as Float"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li3ONq0quwqo"
      },
      "source": [
        "pre_df['diagnosis'] = dia_code # adding DR grades with data type as Int"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frA6yfQQvUt9"
      },
      "source": [
        "pre_df.to_csv('/content/drive/MyDrive/Colab_Notebooks/Case_study2/preprocessed_images.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0029mTc73QAp"
      },
      "source": [
        "pre_df=pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/Case_study2/preprocessed_images.csv',dtype=str)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiNDL9LpB6lj"
      },
      "source": [
        "**Train and test data split up** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bgfb0s3bvivZ"
      },
      "source": [
        "# train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "y = pre_df['diagnosis'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(pre_df, y, test_size=0.20, stratify=y)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfWjSmAgxflr",
        "outputId": "3d8f49a3-6c85-45f5-b0f9-af1ca0e6cc1d"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4188, 4)\n",
            "(1047, 4)\n",
            "(4188,)\n",
            "(1047,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxVaSQARCYp7"
      },
      "source": [
        "**1. VGG Model**\n",
        "\n",
        "* Use VGG-16 pretrained network without Fully Connected layers and initilize all the weights with Imagenet trained weights.\n",
        "* After VGG-16 network without FC layers, use conv layers only as Fully connected layer. Reference - https://cs231n.github.io/convolutional-networks/#convert\n",
        "* Final architecture will be VGG-16 without FC layers(without top), 2 Conv layers identical to FC layers, 1 output layer for 5 class classification. INPUT --> VGG-16 without Top layers(FC) --> 2 Conv Layers identical to FC --> Output Layer\n",
        "* Train only last 2 Conv layers identical to FC layers, 1 output layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUxeWbyFGoMK"
      },
      "source": [
        "**1.1 Image Augumentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnrLY__55tdF",
        "outputId": "a711c267-3d58-415a-9372-c23afcd04df9"
      },
      "source": [
        "# reference URL - https://vijayabhaskar96.medium.com/tutorial-on-keras-flow-from-dataframe-1fd4493d237c\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen=ImageDataGenerator(rescale=1./255., validation_split=0.20, rotation_range=15, fill_mode='nearest', width_shift_range=0.1, height_shift_range=0.1, \n",
        "                horizontal_flip=True, vertical_flip=True)\n",
        "\n",
        "train_generator=datagen.flow_from_dataframe(dataframe=X_train, directory=None, x_col=\"path\", y_col=\"diagnosis\", subset=\"training\", \n",
        "                batch_size=8, seed=10, shuffle=True, class_mode=\"categorical\", drop_duplicates = False, target_size=(320,320))\n",
        "\n",
        "valid_generator=datagen.flow_from_dataframe(dataframe=X_train, directory=None, x_col=\"path\", y_col=\"diagnosis\", subset=\"validation\", \n",
        "                        batch_size=8, seed=10, shuffle=True, class_mode=\"categorical\", drop_duplicates = False, target_size=(320,320))\n",
        "\n",
        "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
        "\n",
        "test_generator=test_datagen.flow_from_dataframe(dataframe=X_test, directory=None, x_col=\"path\", \n",
        "                y_col=None, batch_size=8, seed=10, shuffle=False, class_mode=None, target_size=(320,320))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3351 validated image filenames belonging to 5 classes.\n",
            "Found 837 validated image filenames belonging to 5 classes.\n",
            "Found 1047 validated image filenames.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLOMLb7uG8yD"
      },
      "source": [
        "**1.3 Building VGG16 Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgDHPbVv8DwD",
        "outputId": "803ca05b-bb18-4b86-835a-9e0e44226c20"
      },
      "source": [
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "\n",
        "##https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
        "## Have to clear the session. If you are not clearing, Graph will create again and again and graph size will increses. \n",
        "## Varibles will also set to some value from before session\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "## Set the random seed values to regenerate the model.\n",
        "np.random.seed(0)\n",
        "rn.seed(0)\n",
        "\n",
        "vgg_base = VGG16(include_top=False, weights='imagenet', input_shape=(320,320,3))\n",
        "\n",
        "# Freeze base model\n",
        "vgg_base.trainable = False\n",
        "\n",
        "#Input layer\n",
        "input_layer = Input(shape=(320,320,3))\n",
        "\n",
        "vgg_layer= vgg_base(input_layer)\n",
        "\n",
        "#Conv Layer\n",
        "Conv1 = Conv2D(128, 10, strides=1 , padding = 'valid', activation='relu')(vgg_layer)\n",
        "\n",
        "\n",
        "#MaxPool Layer\n",
        "Conv2 = Conv2D(64, 1, strides=1 , padding = 'valid', activation='relu')(Conv1)\n",
        "\n",
        "flatten = Flatten(data_format='channels_last')(Conv2)\n",
        "#output layer\n",
        "outputs = Dense(5,activation='softmax')(flatten)\n",
        "\n",
        "\n",
        "#Creating a model\n",
        "vgg_model = Model(inputs=input_layer,outputs=outputs)\n",
        "\n",
        "\n",
        "#creating object to ReduceLROnPlateau class to decay lr by 10% If validation accuracy is less than previous epoch accuracy(cond1)\n",
        "reducelrate = tf.keras.callbacks.ReduceLROnPlateau(monitor='accuracy', factor=0.96, verbose=1, patience=2, cooldown=2)\n",
        "\n",
        "#creating object to ModelCheckpoint class to Save model at every epoch if validation accuracy is improved from previous epoch\n",
        "filepath='/content/w/weights-{epoch:02d}-{accuracy:.4f}.hdf5'\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, save_best_only=True, monitor='val_accuracy',  verbose=1)\n",
        "\n",
        "callbacks_list = [reducelrate, checkpoint] #Merging all the callbacks in a list\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "vgg_model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy']) #Compiling model with cross-entropy as loss\n",
        "print(vgg_model.summary())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 320, 320, 3)]     0         \n",
            "_________________________________________________________________\n",
            "vgg16 (Functional)           (None, 10, 10, 512)       14714688  \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 1, 1, 128)         6553728   \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 1, 1, 64)          8256      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 325       \n",
            "=================================================================\n",
            "Total params: 21,276,997\n",
            "Trainable params: 6,562,309\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2ygSNeWHUHl"
      },
      "source": [
        "**1.4 Training the VGG16 model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFJJZCzwA4Tf",
        "outputId": "c71c7b60-ce69-4830-a387-2b35311b2019"
      },
      "source": [
        "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
        "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
        "vgg_model.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN, validation_data=valid_generator,\n",
        "                    validation_steps=STEP_SIZE_VALID, epochs=40, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "392/392 [==============================] - 2702s 7s/step - loss: 1.0720 - accuracy: 0.6042 - val_loss: 0.9942 - val_accuracy: 0.6059\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.60587, saving model to /content/w/weights-01-0.6042.hdf5\n",
            "Epoch 2/40\n",
            "392/392 [==============================] - 86s 219ms/step - loss: 0.9205 - accuracy: 0.6467 - val_loss: 0.9388 - val_accuracy: 0.6531\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.60587 to 0.65306, saving model to /content/w/weights-02-0.6467.hdf5\n",
            "Epoch 3/40\n",
            "392/392 [==============================] - 86s 219ms/step - loss: 0.8916 - accuracy: 0.6451 - val_loss: 0.8613 - val_accuracy: 0.6518\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.65306\n",
            "Epoch 4/40\n",
            "392/392 [==============================] - 86s 219ms/step - loss: 0.8498 - accuracy: 0.6687 - val_loss: 0.9760 - val_accuracy: 0.6237\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.65306\n",
            "Epoch 5/40\n",
            "392/392 [==============================] - 86s 219ms/step - loss: 0.8206 - accuracy: 0.6716 - val_loss: 0.8844 - val_accuracy: 0.6454\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.65306\n",
            "Epoch 6/40\n",
            "392/392 [==============================] - 86s 220ms/step - loss: 0.8073 - accuracy: 0.6776 - val_loss: 0.9435 - val_accuracy: 0.6173\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.65306\n",
            "Epoch 7/40\n",
            "392/392 [==============================] - 86s 219ms/step - loss: 0.8009 - accuracy: 0.6802 - val_loss: 0.8059 - val_accuracy: 0.6696\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.65306 to 0.66964, saving model to /content/w/weights-07-0.6802.hdf5\n",
            "Epoch 8/40\n",
            "392/392 [==============================] - 86s 219ms/step - loss: 0.7566 - accuracy: 0.6984 - val_loss: 0.8280 - val_accuracy: 0.6696\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.66964\n",
            "Epoch 9/40\n",
            "392/392 [==============================] - 86s 220ms/step - loss: 0.7629 - accuracy: 0.7076 - val_loss: 0.7827 - val_accuracy: 0.7015\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.66964 to 0.70153, saving model to /content/w/weights-09-0.7076.hdf5\n",
            "Epoch 10/40\n",
            "392/392 [==============================] - 86s 219ms/step - loss: 0.7603 - accuracy: 0.6977 - val_loss: 0.8008 - val_accuracy: 0.6913\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.70153\n",
            "Epoch 11/40\n",
            "392/392 [==============================] - 86s 220ms/step - loss: 0.7378 - accuracy: 0.6961 - val_loss: 0.7916 - val_accuracy: 0.6773\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.599999757483601e-05.\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.70153\n",
            "Epoch 12/40\n",
            "392/392 [==============================] - 86s 219ms/step - loss: 0.7308 - accuracy: 0.7166 - val_loss: 0.7879 - val_accuracy: 0.6849\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.70153\n",
            "Epoch 13/40\n",
            "392/392 [==============================] - 86s 221ms/step - loss: 0.7196 - accuracy: 0.7156 - val_loss: 0.7884 - val_accuracy: 0.6875\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.70153\n",
            "Epoch 14/40\n",
            "392/392 [==============================] - 86s 219ms/step - loss: 0.7078 - accuracy: 0.7175 - val_loss: 0.7922 - val_accuracy: 0.6811\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.70153\n",
            "Epoch 15/40\n",
            "392/392 [==============================] - 87s 221ms/step - loss: 0.6886 - accuracy: 0.7226 - val_loss: 0.7799 - val_accuracy: 0.6633\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.70153\n",
            "Epoch 16/40\n",
            "392/392 [==============================] - 86s 220ms/step - loss: 0.6794 - accuracy: 0.7322 - val_loss: 0.7980 - val_accuracy: 0.6837\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.70153\n",
            "Epoch 17/40\n",
            "392/392 [==============================] - 86s 220ms/step - loss: 0.6769 - accuracy: 0.7380 - val_loss: 0.7884 - val_accuracy: 0.6888\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.70153\n",
            "Epoch 18/40\n",
            "392/392 [==============================] - 86s 219ms/step - loss: 0.6767 - accuracy: 0.7360 - val_loss: 0.8215 - val_accuracy: 0.6582\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.70153\n",
            "Epoch 19/40\n",
            "392/392 [==============================] - 86s 220ms/step - loss: 0.6523 - accuracy: 0.7424 - val_loss: 0.7772 - val_accuracy: 0.6786\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.70153\n",
            "Epoch 20/40\n",
            "392/392 [==============================] - 86s 219ms/step - loss: 0.6498 - accuracy: 0.7434 - val_loss: 0.8192 - val_accuracy: 0.6875\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.70153\n",
            "Epoch 21/40\n",
            "392/392 [==============================] - 86s 221ms/step - loss: 0.6505 - accuracy: 0.7418 - val_loss: 0.7344 - val_accuracy: 0.7028\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.70153 to 0.70281, saving model to /content/w/weights-21-0.7418.hdf5\n",
            "Epoch 22/40\n",
            "392/392 [==============================] - 87s 222ms/step - loss: 0.6334 - accuracy: 0.7482 - val_loss: 0.7478 - val_accuracy: 0.7245\n",
            "\n",
            "Epoch 00022: val_accuracy improved from 0.70281 to 0.72449, saving model to /content/w/weights-22-0.7482.hdf5\n",
            "Epoch 23/40\n",
            "392/392 [==============================] - 88s 223ms/step - loss: 0.6259 - accuracy: 0.7510 - val_loss: 0.7182 - val_accuracy: 0.7219\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.72449\n",
            "Epoch 24/40\n",
            "392/392 [==============================] - 88s 224ms/step - loss: 0.6240 - accuracy: 0.7536 - val_loss: 0.7686 - val_accuracy: 0.7092\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.72449\n",
            "Epoch 25/40\n",
            "392/392 [==============================] - 88s 224ms/step - loss: 0.6160 - accuracy: 0.7520 - val_loss: 0.7283 - val_accuracy: 0.7219\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.72449\n",
            "Epoch 26/40\n",
            "392/392 [==============================] - 87s 223ms/step - loss: 0.6005 - accuracy: 0.7622 - val_loss: 0.8157 - val_accuracy: 0.6939\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.72449\n",
            "Epoch 27/40\n",
            "392/392 [==============================] - 88s 224ms/step - loss: 0.6069 - accuracy: 0.7555 - val_loss: 0.7587 - val_accuracy: 0.6926\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.72449\n",
            "Epoch 28/40\n",
            "392/392 [==============================] - 87s 223ms/step - loss: 0.5921 - accuracy: 0.7715 - val_loss: 0.7795 - val_accuracy: 0.6901\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.72449\n",
            "Epoch 29/40\n",
            "392/392 [==============================] - 87s 223ms/step - loss: 0.5863 - accuracy: 0.7670 - val_loss: 0.7954 - val_accuracy: 0.6913\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.72449\n",
            "Epoch 30/40\n",
            "392/392 [==============================] - 88s 223ms/step - loss: 0.5871 - accuracy: 0.7644 - val_loss: 0.7613 - val_accuracy: 0.6939\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.215999627485871e-05.\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.72449\n",
            "Epoch 31/40\n",
            "392/392 [==============================] - 87s 222ms/step - loss: 0.5639 - accuracy: 0.7769 - val_loss: 0.7710 - val_accuracy: 0.6952\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.72449\n",
            "Epoch 32/40\n",
            "392/392 [==============================] - 87s 223ms/step - loss: 0.5566 - accuracy: 0.7810 - val_loss: 0.7781 - val_accuracy: 0.6811\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.72449\n",
            "Epoch 33/40\n",
            "392/392 [==============================] - 87s 223ms/step - loss: 0.5541 - accuracy: 0.7791 - val_loss: 0.7529 - val_accuracy: 0.7079\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.72449\n",
            "Epoch 34/40\n",
            "392/392 [==============================] - 87s 223ms/step - loss: 0.5632 - accuracy: 0.7801 - val_loss: 0.7796 - val_accuracy: 0.6811\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 8.847359335049987e-05.\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.72449\n",
            "Epoch 35/40\n",
            "392/392 [==============================] - 88s 223ms/step - loss: 0.5564 - accuracy: 0.7731 - val_loss: 0.7655 - val_accuracy: 0.7066\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.72449\n",
            "Epoch 36/40\n",
            "392/392 [==============================] - 87s 222ms/step - loss: 0.5382 - accuracy: 0.7861 - val_loss: 0.7531 - val_accuracy: 0.7041\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.72449\n",
            "Epoch 37/40\n",
            "392/392 [==============================] - 87s 222ms/step - loss: 0.5299 - accuracy: 0.7823 - val_loss: 0.8193 - val_accuracy: 0.6990\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.72449\n",
            "Epoch 38/40\n",
            "392/392 [==============================] - 87s 223ms/step - loss: 0.5266 - accuracy: 0.7986 - val_loss: 0.7895 - val_accuracy: 0.6888\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.72449\n",
            "Epoch 39/40\n",
            "392/392 [==============================] - 87s 222ms/step - loss: 0.5320 - accuracy: 0.7887 - val_loss: 0.7605 - val_accuracy: 0.6990\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.72449\n",
            "Epoch 40/40\n",
            "392/392 [==============================] - 87s 223ms/step - loss: 0.5123 - accuracy: 0.8027 - val_loss: 0.7820 - val_accuracy: 0.7054\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.72449\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa9f8108c10>"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K34uPV1VHdDn"
      },
      "source": [
        "**1.5 Prediction on Test images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy9QjmpibDsK"
      },
      "source": [
        "# Generate predictions\n",
        "vgg_model.load_weights('/content/w/weights-22-0.7482.hdf5') \n",
        "\n",
        "y_pred = vgg_model.predict(test_generator)\n",
        "y_pred_class = np.argmax(y_pred, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef0rBh7BHlr_"
      },
      "source": [
        "**1.6 Calculating Quadratic cohen kappa score**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4pti9PRDYpp",
        "outputId": "71c5e156-6fd4-4981-e6ab-4e9ac0d189c1"
      },
      "source": [
        "cohen = cohen_kappa_score(y_pred_class, y_test.astype('int'), weights='quadratic')\n",
        "print(\"Quadratic Cohen kappa score of VGG model on test data is - %.3f\" %cohen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Quadratic Cohen kappa score of VGG model on test data is - 0.793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SJ0noWVICW-"
      },
      "source": [
        "**1.7 Calculating Accuracy & Macro F1 score on test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "N29ZBuA9cvOA",
        "outputId": "ddbf7e5b-84f6-413b-960b-c4fc39645e60"
      },
      "source": [
        "print ('Classification Report : \\n', classification_report(y_test.astype('int'), y_pred_class))\n",
        "sns.heatmap(confusion_matrix(y_test.astype('int'), y_pred_class), annot=True, fmt=\"d\");\n",
        "plt.title(\"Confusion matrix\")\n",
        "plt.ylabel('Actual class')\n",
        "plt.xlabel('Predicted class')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report : \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.93      0.89       610\n",
            "           1       0.50      0.21      0.30       151\n",
            "           2       0.61      0.75      0.67       364\n",
            "           3       0.45      0.43      0.44        89\n",
            "           4       0.59      0.25      0.35        95\n",
            "\n",
            "    accuracy                           0.72      1309\n",
            "   macro avg       0.60      0.52      0.53      1309\n",
            "weighted avg       0.70      0.72      0.69      1309\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb5NfWwQIhFl"
      },
      "source": [
        "**2. DenseNet Model**\n",
        "\n",
        "* It connects each layer to every other layer in a feed-forward fashion\n",
        "* Each CNN layers with L layers have L connections - one between each layer and its subsequent layers\n",
        "* For each layer, the feature maps of all preceding layers are used as inputs into all subsequent layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxOIIgJdqbjZ"
      },
      "source": [
        "**2.1 Setting up hyper-parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhFB4F8EfK2D"
      },
      "source": [
        "# Hyperparameters\n",
        "batch_size = 8\n",
        "num_classes = 5\n",
        "l = 14\n",
        "num_filter = 32\n",
        "compression = 0.5\n",
        "dropout_rate = 0"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhQfcorlmMAQ"
      },
      "source": [
        "img_height = 32\n",
        "img_width = 32\n",
        "channel = 3"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNHhVoSSqh6X"
      },
      "source": [
        "**2.2 Defining Dense, Transition & Output block of the DenseNet model** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9OrRE4Il8fs"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "# Dense Block\n",
        "def denseblock(input, num_filter, dropout_rate):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l): \n",
        "        BatchNorm = layers.BatchNormalization()(temp)\n",
        "        relu = layers.Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "\n",
        "## transition Blosck\n",
        "def transition(input, num_filter, dropout_rate):\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    return avg\n",
        "\n",
        "#output layer\n",
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = layers.Flatten()(AvgPooling)\n",
        "    output = layers.Dense(num_classes, activation='softmax')(flat)\n",
        "    return output"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhnMoGMIqyDo"
      },
      "source": [
        "**2.3 Architecure of the DenseNet model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZIWtaGOmCaS"
      },
      "source": [
        "num_filter = 32\n",
        "dropout_rate = 0\n",
        "l = 14\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "input = layers.Input(shape=(img_height, img_width, channel,))\n",
        "First_Conv2D = layers.Conv2D(24, (2,2), use_bias=False ,padding='same', kernel_initializer='he_uniform', bias_initializer='zeros',kernel_regularizer=l2(0.0001))(input)\n",
        "\n",
        "First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
        "\n",
        "Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "Last_Block = denseblock(Third_Transition, num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)"
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNjso0fhmEcQ",
        "outputId": "13f6098a-38ec-4eac-8134-3dd3f08abac3"
      },
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 24)   288         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 24)   96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 24)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 16)   3456        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 32, 32, 40)   0           conv2d[0][0]                     \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 40)   160         concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 40)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 16)   5760        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 56)   0           concatenate[0][0]                \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 56)   224         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 56)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 16)   8064        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 72)   0           concatenate_1[0][0]              \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 72)   288         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 72)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 16)   10368       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 88)   0           concatenate_2[0][0]              \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 88)   352         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 88)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 16)   12672       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 104)  0           concatenate_3[0][0]              \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 104)  416         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 104)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 16)   14976       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 120)  0           concatenate_4[0][0]              \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 120)  480         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 120)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 16)   17280       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 32, 32, 136)  0           concatenate_5[0][0]              \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 136)  544         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 136)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 16)   19584       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 32, 32, 152)  0           concatenate_6[0][0]              \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 152)  608         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 152)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 16)   21888       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 32, 32, 168)  0           concatenate_7[0][0]              \n",
            "                                                                 conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 168)  672         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 168)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 16)   24192       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 32, 32, 184)  0           concatenate_8[0][0]              \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 184)  736         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 184)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 16)   26496       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 32, 32, 200)  0           concatenate_9[0][0]              \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 200)  800         concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 200)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 16)   28800       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 32, 32, 216)  0           concatenate_10[0][0]             \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 32, 32, 216)  864         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 216)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 16)   31104       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 32, 32, 232)  0           concatenate_11[0][0]             \n",
            "                                                                 conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 32, 32, 232)  928         concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 32, 32, 232)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 16)   33408       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 32, 32, 248)  0           concatenate_12[0][0]             \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 32, 32, 248)  992         concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 32, 32, 248)  0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 32, 32, 16)   3968        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 16, 16, 16)   0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 16)   64          average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 16)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 16)   2304        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 16, 16, 32)   0           average_pooling2d[0][0]          \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 32)   128         concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 32)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 16)   4608        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 16, 16, 48)   0           concatenate_14[0][0]             \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 48)   192         concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 48)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 16)   6912        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 16, 16, 64)   0           concatenate_15[0][0]             \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 64)   256         concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 64)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 16)   9216        activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 16, 16, 80)   0           concatenate_16[0][0]             \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 80)   320         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 80)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 16)   11520       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 16, 16, 96)   0           concatenate_17[0][0]             \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 96)   384         concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 96)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 16)   13824       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 16, 16, 112)  0           concatenate_18[0][0]             \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 16, 16, 112)  448         concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 112)  0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 16, 16, 16)   16128       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 16, 16, 128)  0           concatenate_19[0][0]             \n",
            "                                                                 conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 16, 16, 128)  512         concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 128)  0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 16, 16, 16)   18432       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 16, 16, 144)  0           concatenate_20[0][0]             \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 16, 16, 144)  576         concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 144)  0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 16, 16, 16)   20736       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 16, 16, 160)  0           concatenate_21[0][0]             \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 16, 16, 160)  640         concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 160)  0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 16, 16, 16)   23040       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 16, 16, 176)  0           concatenate_22[0][0]             \n",
            "                                                                 conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 16, 16, 176)  704         concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 176)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 16)   25344       activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 16, 16, 192)  0           concatenate_23[0][0]             \n",
            "                                                                 conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 16, 16, 192)  768         concatenate_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 16, 16, 192)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 16, 16, 16)   27648       activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_25 (Concatenate)    (None, 16, 16, 208)  0           concatenate_24[0][0]             \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 16, 16, 208)  832         concatenate_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 16, 16, 208)  0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 16, 16, 16)   29952       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_26 (Concatenate)    (None, 16, 16, 224)  0           concatenate_25[0][0]             \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 16, 16, 224)  896         concatenate_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 16, 16, 224)  0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 16, 16, 16)   32256       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_27 (Concatenate)    (None, 16, 16, 240)  0           concatenate_26[0][0]             \n",
            "                                                                 conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 16, 16, 240)  960         concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 16, 16, 240)  0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 16, 16, 16)   3840        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 8, 8, 16)     0           conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 8, 8, 16)     64          average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 8, 8, 16)     0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 8, 8, 16)     2304        activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 8, 8, 32)     0           average_pooling2d_1[0][0]        \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 8, 8, 32)     128         concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 8, 32)     0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 8, 8, 16)     4608        activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 8, 8, 48)     0           concatenate_28[0][0]             \n",
            "                                                                 conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 8, 8, 48)     192         concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 48)     0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 8, 8, 16)     6912        activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_30 (Concatenate)    (None, 8, 8, 64)     0           concatenate_29[0][0]             \n",
            "                                                                 conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 8, 8, 64)     256         concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 8, 64)     0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 8, 8, 16)     9216        activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_31 (Concatenate)    (None, 8, 8, 80)     0           concatenate_30[0][0]             \n",
            "                                                                 conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 8, 8, 80)     320         concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 8, 80)     0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 8, 8, 16)     11520       activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_32 (Concatenate)    (None, 8, 8, 96)     0           concatenate_31[0][0]             \n",
            "                                                                 conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 8, 8, 96)     384         concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 8, 96)     0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 8, 8, 16)     13824       activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 8, 8, 112)    0           concatenate_32[0][0]             \n",
            "                                                                 conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 8, 8, 112)    448         concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 8, 8, 112)    0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 8, 8, 16)     16128       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 8, 8, 128)    0           concatenate_33[0][0]             \n",
            "                                                                 conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 8, 128)    512         concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 8, 128)    0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 8, 8, 16)     18432       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 8, 8, 144)    0           concatenate_34[0][0]             \n",
            "                                                                 conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 8, 8, 144)    576         concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 8, 144)    0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 8, 8, 16)     20736       activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_36 (Concatenate)    (None, 8, 8, 160)    0           concatenate_35[0][0]             \n",
            "                                                                 conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 8, 8, 160)    640         concatenate_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 8, 8, 160)    0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 8, 8, 16)     23040       activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_37 (Concatenate)    (None, 8, 8, 176)    0           concatenate_36[0][0]             \n",
            "                                                                 conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 8, 8, 176)    704         concatenate_37[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 8, 8, 176)    0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 8, 8, 16)     25344       activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_38 (Concatenate)    (None, 8, 8, 192)    0           concatenate_37[0][0]             \n",
            "                                                                 conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 8, 8, 192)    768         concatenate_38[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 8, 8, 192)    0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 8, 8, 16)     27648       activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_39 (Concatenate)    (None, 8, 8, 208)    0           concatenate_38[0][0]             \n",
            "                                                                 conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 8, 8, 208)    832         concatenate_39[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 8, 8, 208)    0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 8, 8, 16)     29952       activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_40 (Concatenate)    (None, 8, 8, 224)    0           concatenate_39[0][0]             \n",
            "                                                                 conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 8, 8, 224)    896         concatenate_40[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 8, 8, 224)    0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 8, 8, 16)     32256       activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_41 (Concatenate)    (None, 8, 8, 240)    0           concatenate_40[0][0]             \n",
            "                                                                 conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 8, 8, 240)    960         concatenate_41[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 8, 8, 240)    0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 8, 8, 16)     3840        activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 4, 4, 16)     0           conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 4, 4, 16)     64          average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 4, 4, 16)     0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 4, 4, 16)     2304        activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_42 (Concatenate)    (None, 4, 4, 32)     0           average_pooling2d_2[0][0]        \n",
            "                                                                 conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 4, 4, 32)     128         concatenate_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 4, 4, 32)     0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 4, 4, 16)     4608        activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_43 (Concatenate)    (None, 4, 4, 48)     0           concatenate_42[0][0]             \n",
            "                                                                 conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 4, 4, 48)     192         concatenate_43[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 4, 4, 48)     0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 4, 4, 16)     6912        activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_44 (Concatenate)    (None, 4, 4, 64)     0           concatenate_43[0][0]             \n",
            "                                                                 conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 4, 4, 64)     256         concatenate_44[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 4, 4, 64)     0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 4, 4, 16)     9216        activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_45 (Concatenate)    (None, 4, 4, 80)     0           concatenate_44[0][0]             \n",
            "                                                                 conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 4, 4, 80)     320         concatenate_45[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 4, 4, 80)     0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 4, 4, 16)     11520       activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_46 (Concatenate)    (None, 4, 4, 96)     0           concatenate_45[0][0]             \n",
            "                                                                 conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 4, 4, 96)     384         concatenate_46[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 4, 4, 96)     0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 4, 4, 16)     13824       activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_47 (Concatenate)    (None, 4, 4, 112)    0           concatenate_46[0][0]             \n",
            "                                                                 conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 4, 4, 112)    448         concatenate_47[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 4, 4, 112)    0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 4, 4, 16)     16128       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_48 (Concatenate)    (None, 4, 4, 128)    0           concatenate_47[0][0]             \n",
            "                                                                 conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 4, 4, 128)    512         concatenate_48[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 4, 4, 128)    0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 4, 4, 16)     18432       activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_49 (Concatenate)    (None, 4, 4, 144)    0           concatenate_48[0][0]             \n",
            "                                                                 conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 4, 4, 144)    576         concatenate_49[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 4, 4, 144)    0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 4, 4, 16)     20736       activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_50 (Concatenate)    (None, 4, 4, 160)    0           concatenate_49[0][0]             \n",
            "                                                                 conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 4, 4, 160)    640         concatenate_50[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 4, 4, 160)    0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 4, 4, 16)     23040       activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_51 (Concatenate)    (None, 4, 4, 176)    0           concatenate_50[0][0]             \n",
            "                                                                 conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 4, 4, 176)    704         concatenate_51[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 4, 4, 176)    0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 4, 4, 16)     25344       activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_52 (Concatenate)    (None, 4, 4, 192)    0           concatenate_51[0][0]             \n",
            "                                                                 conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 4, 4, 192)    768         concatenate_52[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 4, 4, 192)    0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 4, 4, 16)     27648       activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_53 (Concatenate)    (None, 4, 4, 208)    0           concatenate_52[0][0]             \n",
            "                                                                 conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 4, 4, 208)    832         concatenate_53[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 4, 4, 208)    0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 4, 4, 16)     29952       activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_54 (Concatenate)    (None, 4, 4, 224)    0           concatenate_53[0][0]             \n",
            "                                                                 conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 4, 4, 224)    896         concatenate_54[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 4, 4, 224)    0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 4, 4, 16)     32256       activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_55 (Concatenate)    (None, 4, 4, 240)    0           concatenate_54[0][0]             \n",
            "                                                                 conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 4, 4, 240)    960         concatenate_55[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 4, 4, 240)    0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 2, 2, 240)    0           activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 960)          0           average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 5)            4805        flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,031,749\n",
            "Trainable params: 1,016,149\n",
            "Non-trainable params: 15,600\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxNCiyfQq6Wn"
      },
      "source": [
        "**2.4 Defining the optimizers and compiling the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIr4LpExmdlf"
      },
      "source": [
        "# determine Loss function and Optimizer\n",
        "tf.keras.backend.clear_session()\n",
        "#optimizer = SGD(learning_rate=0.1, momentum=0.9, decay= 1e-6, nesterov=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1UcZK7XrArV"
      },
      "source": [
        "**2.5 Image augumentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_jRutR1mkai",
        "outputId": "8a1ad633-4afa-45fc-8f38-8fbb9a0ebd2f"
      },
      "source": [
        "# reference URL - https://vijayabhaskar96.medium.com/tutorial-on-keras-flow-from-dataframe-1fd4493d237c\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen=ImageDataGenerator(rescale=1./255., validation_split=0.20, horizontal_flip=True)\n",
        "\n",
        "train_generator=datagen.flow_from_dataframe(dataframe=X_train, directory=None, x_col=\"path\", y_col=\"diagnosis\", subset=\"training\", \n",
        "                batch_size=8, shuffle=True, class_mode=\"categorical\", drop_duplicates = False, target_size=(32,32))\n",
        "\n",
        "valid_generator=datagen.flow_from_dataframe(dataframe=X_train, directory=None, x_col=\"path\", y_col=\"diagnosis\", subset=\"validation\", \n",
        "                        batch_size=8, shuffle=True, class_mode=\"categorical\", drop_duplicates = False, target_size=(32,32))\n",
        "\n",
        "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
        "\n",
        "test_generator=test_datagen.flow_from_dataframe(dataframe=X_test, directory=None, x_col=\"path\", \n",
        "                y_col=None, batch_size=8, shuffle=False, class_mode=None, target_size=(32,32))"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3351 validated image filenames belonging to 5 classes.\n",
            "Found 837 validated image filenames belonging to 5 classes.\n",
            "Found 1047 validated image filenames.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzCMNE2gnIvd"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "#creating object to 1Cycle_Learningrate_Scheduler class to reduce lr gradually\n",
        "#lr_manager = OneCycleLR(3351, 16, max_lr=0.01, end_percentage=0.2, scale_percentage=0.1, maximum_momentum=0.90, verbose=True)\n",
        "\n",
        "#creating object to ReduceLROnPlateau class to decay lr by 10% If validation accuracy is less than previous epoch accuracy(cond1)\n",
        "#reducelrate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.90, verbose=1, patience=4, cooldown=1, min_lr=0.000001)\n",
        "\n",
        "#creating object to ModelCheckpoint class to Save model at every epoch if validation accuracy is improved from previous epoch\n",
        "filepath='/content/d/weights-{epoch:02d}-{accuracy:.4f}.hdf5'\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, save_best_only=True, monitor='val_accuracy',  verbose=1)\n",
        "\n",
        "callbacks_list = [checkpoint] #Merging all the callbacks in a list"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dzm3FvBrGBh"
      },
      "source": [
        "**2.6 Training the DenseNet model on Train data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ6qWrXPol7Y",
        "outputId": "492a9a41-c257-4f94-cae3-0026fb874262"
      },
      "source": [
        "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
        "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
        "model.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN, validation_data=valid_generator,\n",
        "                    validation_steps=STEP_SIZE_VALID, epochs=100, callbacks=callbacks_list)"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "418/418 [==============================] - 22s 41ms/step - loss: 1.3021 - accuracy: 0.4954 - val_loss: 1.1810 - val_accuracy: 0.5481\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.54808, saving model to /content/d/weights-01-0.4954.hdf5\n",
            "Epoch 2/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 1.1511 - accuracy: 0.5624 - val_loss: 1.1348 - val_accuracy: 0.5889\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.54808 to 0.58894, saving model to /content/d/weights-02-0.5624.hdf5\n",
            "Epoch 3/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 1.0849 - accuracy: 0.5890 - val_loss: 1.1487 - val_accuracy: 0.5541\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.58894\n",
            "Epoch 4/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 1.0401 - accuracy: 0.5965 - val_loss: 1.0160 - val_accuracy: 0.6334\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.58894 to 0.63341, saving model to /content/d/weights-04-0.5965.hdf5\n",
            "Epoch 5/100\n",
            "418/418 [==============================] - 17s 41ms/step - loss: 0.9874 - accuracy: 0.6165 - val_loss: 1.2443 - val_accuracy: 0.5685\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.63341\n",
            "Epoch 6/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.9427 - accuracy: 0.6306 - val_loss: 0.9912 - val_accuracy: 0.6286\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.63341\n",
            "Epoch 7/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.9149 - accuracy: 0.6410 - val_loss: 0.9430 - val_accuracy: 0.6274\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.63341\n",
            "Epoch 8/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.8597 - accuracy: 0.6724 - val_loss: 0.9858 - val_accuracy: 0.6322\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.63341\n",
            "Epoch 9/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.8389 - accuracy: 0.6730 - val_loss: 1.4210 - val_accuracy: 0.4724\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.63341\n",
            "Epoch 10/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.7842 - accuracy: 0.7021 - val_loss: 1.0152 - val_accuracy: 0.6226\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.63341\n",
            "Epoch 11/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.7330 - accuracy: 0.7179 - val_loss: 1.0830 - val_accuracy: 0.6334\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.63341\n",
            "Epoch 12/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.7165 - accuracy: 0.7317 - val_loss: 1.5860 - val_accuracy: 0.4026\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.63341\n",
            "Epoch 13/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.6580 - accuracy: 0.7538 - val_loss: 1.2385 - val_accuracy: 0.5252\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.63341\n",
            "Epoch 14/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.6306 - accuracy: 0.7631 - val_loss: 1.0280 - val_accuracy: 0.6202\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.63341\n",
            "Epoch 15/100\n",
            "418/418 [==============================] - 17s 39ms/step - loss: 0.5759 - accuracy: 0.7918 - val_loss: 1.0820 - val_accuracy: 0.6418\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.63341 to 0.64183, saving model to /content/d/weights-15-0.7918.hdf5\n",
            "Epoch 16/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.5302 - accuracy: 0.8026 - val_loss: 1.1448 - val_accuracy: 0.6502\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.64183 to 0.65024, saving model to /content/d/weights-16-0.8026.hdf5\n",
            "Epoch 17/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.4461 - accuracy: 0.8382 - val_loss: 1.4304 - val_accuracy: 0.5517\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.65024\n",
            "Epoch 18/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.4375 - accuracy: 0.8480 - val_loss: 1.1085 - val_accuracy: 0.6454\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.65024\n",
            "Epoch 19/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.3858 - accuracy: 0.8642 - val_loss: 1.4731 - val_accuracy: 0.6094\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.65024\n",
            "Epoch 20/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.3376 - accuracy: 0.8809 - val_loss: 1.5631 - val_accuracy: 0.5577\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.65024\n",
            "Epoch 21/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.3391 - accuracy: 0.8863 - val_loss: 1.4205 - val_accuracy: 0.5805\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.65024\n",
            "Epoch 22/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.3032 - accuracy: 0.9061 - val_loss: 1.7729 - val_accuracy: 0.5216\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.65024\n",
            "Epoch 23/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.2771 - accuracy: 0.9091 - val_loss: 1.5198 - val_accuracy: 0.6058\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.65024\n",
            "Epoch 24/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.2682 - accuracy: 0.9091 - val_loss: 1.6331 - val_accuracy: 0.6082\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.65024\n",
            "Epoch 25/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.2655 - accuracy: 0.9138 - val_loss: 1.8004 - val_accuracy: 0.5216\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.65024\n",
            "Epoch 26/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.2385 - accuracy: 0.9192 - val_loss: 1.4378 - val_accuracy: 0.5925\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.65024\n",
            "Epoch 27/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.2313 - accuracy: 0.9261 - val_loss: 1.5579 - val_accuracy: 0.5817\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.65024\n",
            "Epoch 28/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.2141 - accuracy: 0.9354 - val_loss: 1.5188 - val_accuracy: 0.5901\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.65024\n",
            "Epoch 29/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.2059 - accuracy: 0.9381 - val_loss: 1.6160 - val_accuracy: 0.5962\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.65024\n",
            "Epoch 30/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.2269 - accuracy: 0.9261 - val_loss: 1.4004 - val_accuracy: 0.6262\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.65024\n",
            "Epoch 31/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1907 - accuracy: 0.9399 - val_loss: 1.7560 - val_accuracy: 0.5805\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.65024\n",
            "Epoch 32/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1852 - accuracy: 0.9471 - val_loss: 1.6455 - val_accuracy: 0.6226\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.65024\n",
            "Epoch 33/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1716 - accuracy: 0.9471 - val_loss: 1.7967 - val_accuracy: 0.5637\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.65024\n",
            "Epoch 34/100\n",
            "418/418 [==============================] - 17s 41ms/step - loss: 0.1851 - accuracy: 0.9411 - val_loss: 2.0239 - val_accuracy: 0.4868\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.65024\n",
            "Epoch 35/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.1853 - accuracy: 0.9444 - val_loss: 1.8082 - val_accuracy: 0.6058\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.65024\n",
            "Epoch 36/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.1899 - accuracy: 0.9459 - val_loss: 1.5982 - val_accuracy: 0.6202\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.65024\n",
            "Epoch 37/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.1743 - accuracy: 0.9462 - val_loss: 1.6910 - val_accuracy: 0.5853\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.65024\n",
            "Epoch 38/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.1544 - accuracy: 0.9545 - val_loss: 1.7650 - val_accuracy: 0.5697\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.65024\n",
            "Epoch 39/100\n",
            "418/418 [==============================] - 17s 39ms/step - loss: 0.1502 - accuracy: 0.9512 - val_loss: 2.0052 - val_accuracy: 0.5180\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.65024\n",
            "Epoch 40/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.1562 - accuracy: 0.9512 - val_loss: 1.7707 - val_accuracy: 0.5745\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.65024\n",
            "Epoch 41/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.1454 - accuracy: 0.9521 - val_loss: 1.8565 - val_accuracy: 0.6118\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.65024\n",
            "Epoch 42/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.1543 - accuracy: 0.9524 - val_loss: 1.6859 - val_accuracy: 0.5974\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.65024\n",
            "Epoch 43/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.1776 - accuracy: 0.9453 - val_loss: 1.7285 - val_accuracy: 0.6394\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.65024\n",
            "Epoch 44/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.1566 - accuracy: 0.9548 - val_loss: 1.7086 - val_accuracy: 0.6070\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.65024\n",
            "Epoch 45/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.1441 - accuracy: 0.9575 - val_loss: 1.9009 - val_accuracy: 0.5288\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.65024\n",
            "Epoch 46/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.1358 - accuracy: 0.9587 - val_loss: 1.7211 - val_accuracy: 0.6082\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.65024\n",
            "Epoch 47/100\n",
            "418/418 [==============================] - 17s 39ms/step - loss: 0.1279 - accuracy: 0.9617 - val_loss: 1.8314 - val_accuracy: 0.6094\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.65024\n",
            "Epoch 48/100\n",
            "418/418 [==============================] - 17s 40ms/step - loss: 0.1160 - accuracy: 0.9659 - val_loss: 1.8188 - val_accuracy: 0.5421\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.65024\n",
            "Epoch 49/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1145 - accuracy: 0.9614 - val_loss: 2.0032 - val_accuracy: 0.5829\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.65024\n",
            "Epoch 50/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1726 - accuracy: 0.9414 - val_loss: 1.8391 - val_accuracy: 0.5962\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.65024\n",
            "Epoch 51/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1457 - accuracy: 0.9518 - val_loss: 1.7813 - val_accuracy: 0.5913\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.65024\n",
            "Epoch 52/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1126 - accuracy: 0.9656 - val_loss: 1.8444 - val_accuracy: 0.6070\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.65024\n",
            "Epoch 53/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1194 - accuracy: 0.9656 - val_loss: 1.9931 - val_accuracy: 0.6046\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.65024\n",
            "Epoch 54/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1295 - accuracy: 0.9590 - val_loss: 1.6981 - val_accuracy: 0.6334\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.65024\n",
            "Epoch 55/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1131 - accuracy: 0.9629 - val_loss: 1.9565 - val_accuracy: 0.5637\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.65024\n",
            "Epoch 56/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1252 - accuracy: 0.9596 - val_loss: 1.9802 - val_accuracy: 0.5469\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.65024\n",
            "Epoch 57/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1135 - accuracy: 0.9632 - val_loss: 2.2516 - val_accuracy: 0.6154\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.65024\n",
            "Epoch 58/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1142 - accuracy: 0.9638 - val_loss: 2.2655 - val_accuracy: 0.6274\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.65024\n",
            "Epoch 59/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1483 - accuracy: 0.9527 - val_loss: 1.8157 - val_accuracy: 0.5841\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.65024\n",
            "Epoch 60/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1071 - accuracy: 0.9641 - val_loss: 2.0835 - val_accuracy: 0.5913\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.65024\n",
            "Epoch 61/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1073 - accuracy: 0.9626 - val_loss: 1.9891 - val_accuracy: 0.5877\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.65024\n",
            "Epoch 62/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1157 - accuracy: 0.9596 - val_loss: 1.9520 - val_accuracy: 0.5793\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.65024\n",
            "Epoch 63/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1081 - accuracy: 0.9632 - val_loss: 2.2464 - val_accuracy: 0.5204\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.65024\n",
            "Epoch 64/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.0935 - accuracy: 0.9704 - val_loss: 2.1017 - val_accuracy: 0.5601\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.65024\n",
            "Epoch 65/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1084 - accuracy: 0.9617 - val_loss: 1.8959 - val_accuracy: 0.6142\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.65024\n",
            "Epoch 66/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1178 - accuracy: 0.9563 - val_loss: 1.9802 - val_accuracy: 0.5805\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.65024\n",
            "Epoch 67/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1243 - accuracy: 0.9533 - val_loss: 1.7982 - val_accuracy: 0.6310\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.65024\n",
            "Epoch 68/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.0963 - accuracy: 0.9662 - val_loss: 1.9573 - val_accuracy: 0.5853\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.65024\n",
            "Epoch 69/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.0985 - accuracy: 0.9638 - val_loss: 2.0155 - val_accuracy: 0.6262\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.65024\n",
            "Epoch 70/100\n",
            "418/418 [==============================] - 17s 39ms/step - loss: 0.0998 - accuracy: 0.9665 - val_loss: 1.8693 - val_accuracy: 0.6034\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.65024\n",
            "Epoch 71/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.0881 - accuracy: 0.9686 - val_loss: 2.2014 - val_accuracy: 0.5589\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.65024\n",
            "Epoch 72/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.0909 - accuracy: 0.9668 - val_loss: 2.3206 - val_accuracy: 0.5745\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.65024\n",
            "Epoch 73/100\n",
            "418/418 [==============================] - 17s 39ms/step - loss: 0.0993 - accuracy: 0.9656 - val_loss: 2.0670 - val_accuracy: 0.5817\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.65024\n",
            "Epoch 74/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1088 - accuracy: 0.9617 - val_loss: 2.0283 - val_accuracy: 0.5962\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.65024\n",
            "Epoch 75/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1035 - accuracy: 0.9650 - val_loss: 2.0291 - val_accuracy: 0.6058\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.65024\n",
            "Epoch 76/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1160 - accuracy: 0.9584 - val_loss: 2.0358 - val_accuracy: 0.6010\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.65024\n",
            "Epoch 77/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.0973 - accuracy: 0.9647 - val_loss: 1.9172 - val_accuracy: 0.5950\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.65024\n",
            "Epoch 78/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.0848 - accuracy: 0.9689 - val_loss: 2.3322 - val_accuracy: 0.5829\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.65024\n",
            "Epoch 79/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.0840 - accuracy: 0.9674 - val_loss: 2.1007 - val_accuracy: 0.6250\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.65024\n",
            "Epoch 80/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1047 - accuracy: 0.9623 - val_loss: 2.1993 - val_accuracy: 0.6142\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.65024\n",
            "Epoch 81/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.0939 - accuracy: 0.9641 - val_loss: 2.0808 - val_accuracy: 0.5841\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.65024\n",
            "Epoch 82/100\n",
            "418/418 [==============================] - 17s 39ms/step - loss: 0.1091 - accuracy: 0.9581 - val_loss: 2.0910 - val_accuracy: 0.5974\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.65024\n",
            "Epoch 83/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.0977 - accuracy: 0.9629 - val_loss: 2.2622 - val_accuracy: 0.5901\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.65024\n",
            "Epoch 84/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.0837 - accuracy: 0.9683 - val_loss: 2.0155 - val_accuracy: 0.6094\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.65024\n",
            "Epoch 85/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.0724 - accuracy: 0.9722 - val_loss: 2.4702 - val_accuracy: 0.5950\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.65024\n",
            "Epoch 86/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.0883 - accuracy: 0.9653 - val_loss: 2.2603 - val_accuracy: 0.5553\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.65024\n",
            "Epoch 87/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.0973 - accuracy: 0.9638 - val_loss: 2.2276 - val_accuracy: 0.5781\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.65024\n",
            "Epoch 88/100\n",
            "418/418 [==============================] - 17s 39ms/step - loss: 0.0891 - accuracy: 0.9701 - val_loss: 2.0849 - val_accuracy: 0.6034\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.65024\n",
            "Epoch 89/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.0967 - accuracy: 0.9644 - val_loss: 2.2066 - val_accuracy: 0.5974\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.65024\n",
            "Epoch 90/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.0911 - accuracy: 0.9662 - val_loss: 2.2752 - val_accuracy: 0.5877\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.65024\n",
            "Epoch 91/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.0632 - accuracy: 0.9743 - val_loss: 2.1355 - val_accuracy: 0.5781\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.65024\n",
            "Epoch 92/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.0920 - accuracy: 0.9632 - val_loss: 2.3015 - val_accuracy: 0.6082\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.65024\n",
            "Epoch 93/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.0883 - accuracy: 0.9647 - val_loss: 2.2506 - val_accuracy: 0.6118\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.65024\n",
            "Epoch 94/100\n",
            "418/418 [==============================] - 17s 39ms/step - loss: 0.0637 - accuracy: 0.9737 - val_loss: 2.0850 - val_accuracy: 0.6250\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.65024\n",
            "Epoch 95/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.1020 - accuracy: 0.9608 - val_loss: 2.2044 - val_accuracy: 0.5974\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.65024\n",
            "Epoch 96/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.0847 - accuracy: 0.9662 - val_loss: 2.3356 - val_accuracy: 0.5685\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.65024\n",
            "Epoch 97/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.0923 - accuracy: 0.9647 - val_loss: 2.3371 - val_accuracy: 0.5565\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.65024\n",
            "Epoch 98/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.0751 - accuracy: 0.9671 - val_loss: 2.4097 - val_accuracy: 0.5938\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.65024\n",
            "Epoch 99/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.0729 - accuracy: 0.9674 - val_loss: 2.4301 - val_accuracy: 0.5673\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.65024\n",
            "Epoch 100/100\n",
            "418/418 [==============================] - 16s 39ms/step - loss: 0.0931 - accuracy: 0.9608 - val_loss: 2.1417 - val_accuracy: 0.6106\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.65024\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe3c842ef50>"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_irqcWdrZSd"
      },
      "source": [
        "**2.7 Predicting Y labels for the test images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIXsV7W3pE1y"
      },
      "source": [
        "# Generate predictions\n",
        "#model.load_weights('/content/d/weights-137-0.6710.hdf5') \n",
        "y_pred = model.predict(test_generator)\n",
        "y_pred_class = np.argmax(y_pred, axis=1)"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfYCaLO6rjn2"
      },
      "source": [
        "**2.8 Calculation of the quadratic cohen kappa score on test data using DenseNet model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Arr2i3EEJH-O",
        "outputId": "fb948f8c-9858-4f8a-91ae-ab33e05248be"
      },
      "source": [
        "cohen = cohen_kappa_score(y_pred_class, y_test.astype(int), weights='quadratic')\n",
        "print(\"Quadratic Cohen kappa score of VGG model on test data is - %.3f\" %cohen)"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Quadratic Cohen kappa score of VGG model on test data is - 0.487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pTKtQTMruZf"
      },
      "source": [
        "**2.9 Calculation of the Macro F1 score on test data using DenseNet model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmRe-yJ9JlU2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "outputId": "dc332c83-315e-4a6b-d2e2-c1eab60946a6"
      },
      "source": [
        "print ('Classification Report : \\n', classification_report(y_test.astype('int'), y_pred_class))\n",
        "sns.heatmap(confusion_matrix(y_test.astype('int'), y_pred_class), annot=True, fmt=\"d\");\n",
        "plt.title(\"Confusion matrix\")\n",
        "plt.ylabel('Actual class')\n",
        "plt.xlabel('Predicted class')\n",
        "plt.show()"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report : \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.69      0.74       488\n",
            "           1       0.24      0.28      0.26       120\n",
            "           2       0.46      0.58      0.51       292\n",
            "           3       0.19      0.21      0.20        71\n",
            "           4       0.25      0.11      0.15        76\n",
            "\n",
            "    accuracy                           0.54      1047\n",
            "   macro avg       0.39      0.37      0.37      1047\n",
            "weighted avg       0.55      0.54      0.54      1047\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDO9sSAQ1bV-",
        "outputId": "63912a6c-888c-40ae-b74d-d8c77a8c651f"
      },
      "source": [
        "print('''-----------------------------------------------------------------------------------------------------------------------\n",
        "| Layers                                        | Test Accuracy            | Test Kappa score  |  Test Macro-F1 score\n",
        "-----------------------------------------------------------------------------------------------------------------------''')\n",
        "print(\"| VGG16 without Top & CNN as FC                 | 0.72                     | 0.79              | 0.53\")\n",
        "print('''-----------------------------------------------------------------------------------------------------------------------''')\n",
        "print(\"| DenseNet                                      | 0.53                     | 0.49              | 0.37\")\n",
        "print('''-----------------------------------------------------------------------------------------------------------------------''')"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------------------------------------\n",
            "| Layers                                        | Test Accuracy            | Test Kappa score  |  Test Macro-F1 score\n",
            "-----------------------------------------------------------------------------------------------------------------------\n",
            "| VGG16 without Top & CNN as FC                 | 0.72                     | 0.79              | 0.53\n",
            "-----------------------------------------------------------------------------------------------------------------------\n",
            "| DenseNet                                      | 0.53                     | 0.49              | 0.37\n",
            "-----------------------------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1zbc4Dks6bz"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        "* VGG16-CNN model is getting overfit from the 30th epoch, and there is no improvement in accuracy and loss. We achieved 72 % accuracy on test data.\n",
        "\n",
        "* DenseNet model is getting overfit after few epochs itself. At end of the 100th epoch it reaches a very good accuracy ~ 95 % on training data, but there is no improvement in test accuracy, and it didn't improve from ~65 %\n",
        "\n",
        "* I have tried with different growth rate with different depths, and many augumentation techniques, and different learning rates with One cycle LR, but none of them produced good results. All of them overfit well."
      ]
    }
  ]
}